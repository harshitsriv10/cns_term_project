# -*- coding: utf-8 -*-
"""cns_doc2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQk0xG9xlH7UdiS1-TNySX0jQE_jSiPm
"""

from google.colab import drive
drive.mount('/content/drive')

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')

import json
f = open('/content/drive/My Drive/cns/data.json',)
data = json.load(f)
f.close()

mails = pd.read_csv("/content/drive/My Drive/spam_ham_dataset.csv", header=0)
mails.head()

data_train, data_test, label_train, label_test = train_test_split(mails['text'], mails['label'],test_size=.10)

tagged_mails = []
for mail_text, label in zip(data_train, label_train):
  tagged_mail = TaggedDocument(words=word_tokenize(mail_text.lower()), tags=[label])
  tagged_mails.append(tagged_mail)

stop_words = set(stopwords.words('english'))

max_epochs = 50
vec_size = 100
alpha_val = 0.025
min_word_count = 1

model = Doc2Vec(vector_size=vec_size, alpha=alpha_val, 
                min_count=min_word_count)
model.build_vocab(tagged_mails)
model.train(tagged_mails, total_examples=model.corpus_count,
             epochs=max_epochs)

model.save("/content/drive/My Drive/cns/d2v.model")

mail_vectors = []
count_list_train = []
for text, label in zip(data_train, label_train):
  mail_text = word_tokenize(text.lower())
  count = 0
  for word in mail_text:
    if word in stop_words:
      continue
    if not word.isalpha():
      continue
    if len(word) <= 1:
      continue
    if word in data:
      count=count+1
  count_list_train.append(count)
  mail_vector = model.infer_vector(mail_text)
  mail_vectors.append(mail_vector)

test_vectors = []
count_list_test = []
for text, label in zip(data_test, label_test):
  test_text = word_tokenize(text.lower())
  count = 0
  for word in test_text:
    if word in stop_words:
      continue
    if not word.isalpha():
      continue
    if len(word) <= 1:
      continue
    if word in data:
      count=count+1
  count_list_test.append(count)
  test_vector = model.infer_vector(test_text)
  test_vectors.append(test_vector)

df = pd.DataFrame(data=mail_vectors)
df['spamcount']=count_list_train
df.head()

df_test=pd.DataFrame(data=test_vectors)
df_test['spamcount']=count_list_test
df_test.head()

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier(max_depth=10, random_state=0)
rf.fit(df,label_train)

lr = LogisticRegression(random_state=0).fit(df,label_train)

sv = SVC()
sv.fit(df,label_train)

test_predict_rf = rf.predict(df_test)
accuracy_score(test_predict_rf,label_test)

test_predict_lr = lr.predict(df_test)
accuracy_score(test_predict_lr,label_test)

test_predict_sv = sv.predict(df_test)
accuracy_score(test_predict_sv,label_test)

correctly_classified = 0
incorrect_spam = 0
incorrect_ham = 0
for text, label in zip(data_test, label_test):
  mail_text = word_tokenize(text.lower())
  mail_vector = model.infer_vector(mail_text)
  predicted_label = model.docvecs.most_similar([mail_vector], topn = 1)
  predicted_label = predicted_label[0][0]
  if predicted_label == label:
    correctly_classified += 1
  elif label == "spam":
    incorrect_spam += 1
  else:
    incorrect_ham += 1

print("Correct = " + str(correctly_classified))
print("Wrong Spam = " + str(incorrect_spam))
print("Wrong Ham = " + str(incorrect_ham))



