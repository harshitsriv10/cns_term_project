# -*- coding: utf-8 -*-
"""list_of_spam_words.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_4uqGGkomJf75ZF4WeHNbDOOEqCtslpE
"""

from nltk.tokenize import word_tokenize
import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')

mails = pd.read_csv("/content/spam_ham_dataset.csv", header=0)
mails.head()

data_train, data_test, label_train, label_test = train_test_split(mails['text'], mails['label'],test_size=.10)

word_count = {}
stop_words = set(stopwords.words('english')) 

for data, label in zip(data_train, label_train):
  if label == "ham":
    continue
  words = word_tokenize(data)
  for word in words:
    if word in stop_words:
      continue
    if not word.isalpha():
      continue
    if len(word) <= 1:
      continue
    if word in word_count.keys():
      word_count[word] += 1
    else:
      word_count[word] = 1

word_count = list(word_count.items())
word_count.sort(key = lambda elem: elem[1], reverse=True)

word_count[1:51]

list = []
for word in word_count[1:51]:
  list.append(word[0])

import json
with open('data.json', 'w') as f:
    json.dump(list, f)

